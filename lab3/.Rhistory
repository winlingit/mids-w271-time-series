# best model seasonal orders: P = 2, Q = 3, AIC = 42.6
# 1. EDA for unemployment rate
unemp = read.csv("UNRATENSA.csv")
head(unemp); tail(unemp)
unemp.ts = ts(unemp$UNRATENSA, start = c(1948, 1), frequency=12)
# split data into training and test sets
unemp.train = window(unemp.ts, start = c(1948, 1), end=c(2015,12), frequency=12)
unemp.test = window(unemp.ts, start=c(2016,1), frequency=12)
# plot raw data
plot.ts(unemp.train, main = 'Monthly unemployment rate, January 1948 - June 2017', ylab = 'Unemployment rate (%)')
acf2(unemp.train, 130)  # ACF: trend, PACF: seasonality
### 2. transform data
# log transform to stabilize variance
lu = log(unemp.train)
acf2(lu, 130)
# diff by 1 lag to remove monthly trend
dlu = diff(lu)
acf2(dlu, 130)  # ACF: seasonal spike, PACF: cutoff in seasonal spike after 3 lags
# diff by 12 lags to remove seasonal trend
ddlu = diff(dlu, lag=12)
acf2(ddlu, 250)  # ACF: cutoff after 1 lag, PACF: cutoff in seasonal spike after 3 lags
# plot transformed data
plot.ts(cbind(unemp.ts,lu,dlu,ddlu), main='Monthly unemployment rate, January 1948 - June 2017')
# 1. EDA for unemployment rate
unemp = read.csv("UNRATENSA.csv")
head(unemp); tail(unemp)
unemp.ts = ts(unemp$UNRATENSA, start = c(1948, 1), frequency=12)
# plot raw data
plot.ts(unemp.ts, main = 'Monthly unemployment rate, January 1948 - June 2017', ylab = 'Unemployment rate (%)')
acf2(unemp.ts, 130)  # ACF: trend, PACF: seasonality
### 2. transform data
# log transform to stabilize variance
lu = log(unemp.ts)
acf2(lu, 130)
# diff by 1 lag to remove monthly trend
dlu = diff(lu)
acf2(dlu, 13)
acf2(dlu, 130)  # ACF: seasonal spike, PACF: cutoff in seasonal trend after 3 lags
# diff by 12 lags to remove seasonal trend
ddlu = diff(dlu, lag=12)
acf2(ddlu, 250)  # ACF: cutoff after 1 lag, PACF: cutoff in seasonal trend after 3 lags
# plot transformed data
plot.ts(cbind(unemp.ts,lu,dlu,ddlu), main='Monthly unemployment rate, January 1948 - June 2017')
# 1. EDA for unemployment rate
unemp = read.csv("UNRATENSA.csv")
head(unemp); tail(unemp)
unemp.ts = ts(unemp$UNRATENSA, start = c(1948, 1), frequency=12)
# plot raw data
plot.ts(unemp.ts, main = 'Monthly unemployment rate, January 1948 - June 2017', ylab = 'Unemployment rate (%)')
acf2(unemp.ts, 130)  # ACF: trend, PACF: seasonality
### 2. transform data
# log transform to stabilize variance
lu = log(unemp.ts)
acf2(lu, 13)
acf2(lu, 130)
# diff by 1 lag to remove monthly trend
dlu = diff(lu)
acf2(dlu, 13)
acf2(dlu, 130)  # ACF: seasonal spike, PACF: cutoff in seasonal trend after 3 lags
# diff by 12 lags to remove seasonal trend
ddlu = diff(dlu, lag=12)
acf2(ddlu, 250)  # ACF: cutoff after 1 lag, PACF: cutoff in seasonal trend after 3 lags
# plot transformed data
plot.ts(cbind(unemp.ts,lu,dlu,ddlu), main='Monthly unemployment rate, January 1948 - June 2017')
# 1. EDA for unemployment rate
unemp = read.csv("UNRATENSA.csv")
head(unemp); tail(unemp)
unemp.ts = ts(unemp$UNRATENSA, start = c(1948, 1), frequency=12)
# plot raw data
plot.ts(unemp.ts, main = 'Monthly unemployment rate, January 1948 - June 2017', ylab = 'Unemployment rate (%)')
acf2(unemp.ts, 130)  # ACF: trend, PACF: seasonality
### 2. transform data
# log transform to stabilize variance
lu = log(unemp.ts)
acf2(lu, 13)
acf2(lu, 130)
# diff by 1 lag to remove monthly trend
dlu = diff(lu)
acf2(dlu, 13)
acf2(dlu, 130)  # ACF: seasonal spike, PACF: cutoff in seasonal trend after 3 lags
# diff by 12 lags to remove seasonal trend
ddlu = diff(dlu, lag=12)
acf2(ddlu, 130)  # ACF: cutoff after 1 lag, PACF: cutoff in seasonal trend after 3 lags
# plot transformed data
plot.ts(cbind(unemp.ts,lu,dlu,ddlu), main='Monthly unemployment rate, January 1948 - June 2017')
# 1. EDA for unemployment rate
unemp = read.csv("UNRATENSA.csv")
head(unemp); tail(unemp)
unemp.ts = ts(unemp$UNRATENSA, start = c(1948, 1), frequency=12)
# plot raw data
plot.ts(unemp.ts, main = 'Monthly unemployment rate, January 1948 - June 2017', ylab = 'Unemployment rate (%)')
acf2(unemp.ts, 130)  # ACF: trend, PACF: seasonality
### 2. transform data
# log transform to stabilize variance
lu = log(unemp.ts)
acf2(lu, 130)
# diff by 1 lag to remove monthly trend
dlu = diff(lu)
acf2(dlu, 130)  # ACF: seasonal spike, PACF: cutoff in seasonal trend after 3 lags
# diff by 12 lags to remove seasonal trend
ddlu = diff(dlu, lag=12)
acf2(ddlu, 130)  # ACF: cutoff after 1 lag, PACF: cutoff in seasonal trend after 3 lags
# plot transformed data
plot.ts(cbind(unemp.ts,lu,dlu,ddlu), main='Monthly unemployment rate, January 1948 - June 2017')
?Arima
?sarima
# find AR, MA orders p, q
# try models w/ max of 3 non-seasonal lags based on ACF, PACF
for (p in 0:3) {
for(q in 0:3) {
fit = Arima(train, order=c(p,1,q), seasonal=list(order=c(2,1,3)), method='CSS-ML', optim.method='BFGS')
print(c(p,q,fit$aic))
}
}
# find AR, MA orders p, q
# try models w/ max of 3 non-seasonal lags based on ACF, PACF
# for (p in 0:3) {
#         for(q in 0:3) {
#                 fit = Arima(train, order=c(p,1,q), seasonal=list(order=c(2,1,3)), method='CSS-ML', optim.method='BFGS')
#                 print(c(p,q,fit$aic))
#         }
# }
# best model orders: p = 1, Q = 2, AIC = -17.0
# ### modeling with astsa
for (p in 0:3) {
for(q in 0:3) {
fit = sarima(train, p,1,q, 2,1,3, 12)
print(c(p,q,fit$AIC))
}
}
# find AR, MA orders p, q
# try models w/ max of 3 non-seasonal lags based on ACF, PACF
# for (p in 0:3) {
#         for(q in 0:3) {
#                 fit = Arima(train, order=c(p,1,q), seasonal=list(order=c(2,1,3)), method='CSS-ML', optim.method='BFGS')
#                 print(c(p,q,fit$aic))
#         }
# }
# best model orders: p = 1, Q = 2, AIC = -17.0
# ### modeling with astsa
for (p in 0:3) {
for(q in 0:3) {
fit = sarima(train, p,1,q, 2,1,3, 12, details=F)
print(c(p,q,fit$AIC))
}
}
### 4. check candidate models for residuals ~ white noise
# SARIMA(1,1,2) (2,1,3)
fit1 = Arima(train, order=c(1,1,2), seasonal=list(order=c(2,1,3)), method='CSS-ML', optim.method='BFGS')
?forecast.Arima
### 4. check candidate models for residuals ~ white noise
# SARIMA(1,1,2) (2,1,3)
fit1 = Arima(train, order=c(1,1,2), seasonal=list(order=c(2,1,3)), method='CSS-ML', optim.method='BFGS')
hist(fit1$residuals)
acf2(fit1$residuals, 130)
>>>>>>> 5c28f1abd90e47b9db2de643f9658ad96f77cd01
knitr::opts_chunk$set(echo = TRUE)
#knitr::opts_knit$set(root.dir = "C:/Users/kctob/OneDrive/Documents/Berkeley/271/mids-w271-time-series/lab2")
library(car)
library(stargazer)
library(Hmisc)
library(effects)
library(astsa)
library(forecast)
library(vars)
library(tseries)
### Forecasts
f1 <- predict(var1, n.ahead = length(unemp.test))
### Forecasts
f1 <- predict(var1, n.ahead = length(unemp.test))
### Forecasts
f1 <- predict(var1, n.ahead = length(unemp.test))
knitr::opts_chunk$set(echo = TRUE)
#knitr::opts_knit$set(root.dir = "C:/Users/kctob/OneDrive/Documents/Berkeley/271/mids-w271-time-series/lab2")
library(car)
library(stargazer)
library(Hmisc)
library(effects)
library(astsa)
library(forecast)
library(vars)
library(tseries)
### 1. EDA for unemployment rate
unemp = read.csv("UNRATENSA.csv")
cbind(head(unemp),tail(unemp))
unemp.ts = ts(unemp$UNRATENSA, start = c(1948, 1), frequency=12)
# plot series
plot.ts(unemp.ts, main = 'Monthly unemployment rate, January 1948 - June 2017', ylab = 'Unemployment rate (%)')
invisible(acf2(unemp.ts, 130))  # ACF: trend, PACF: seasonality
# test null hypothesis of non-stationarity (unit root)
adf.test(unemp.ts)  # p = 0.33 > 0.05, series is non-stationary
### 2. transform data to stationary series
# log transform to stabilize variance
lu = log(unemp.ts)
# diff by 1 lag to remove monthly trend
dlu = diff(lu)
invisible(acf2(dlu, 130))
# ACF: seasonal trend tails off --> seasonal MA
# PACF: cutoff after 3 seasonal lags --> seasonal AR, max order 3
# test null hypothesis of non-stationarity (unit root)
adf.test(dlu)
# p = 0.01 < 0.05 --> series is stationary, but seasonal MA from PACF
# diff by 12 lags to remove seasonality
ddlu = diff(dlu, lag=12)
invisible(acf2(ddlu, 130))
# ACF: cutoff after 1 seasonal lag --> seasonal MA, max order 1
# ACF: cutoff after 3 lags --> MA, max order 3
# PACF: cutoff after 3 seasonal lags --> seasonal
# test null hypothesis of non-stationarity (unit root)
adf.test(ddlu)
# p =0.01 < 0.05, series is stationary
# plot transformed data
plot.ts(cbind(unemp.ts,dlu,ddlu), main='Monthly unemployment rate, January 1948 - June 2017')
# start w/ d=1, D=1 from ADF tests for stationarity, max of 3 seasonal and 3 non-seasonal lags from ACF, PACF
### 2. transform data to stationary series
# log transform to stabilize variance
lu = log(unemp.ts)
# diff by 1 lag to remove monthly trend
dlu = diff(lu)
invisible(acf2(dlu, 130))
# ACF: seasonal trend tails off --> seasonal MA
# PACF: cutoff after 3 seasonal lags --> seasonal AR, max order 3
# test null hypothesis of non-stationarity (unit root)
adf.test(dlu)
# p = 0.01 < 0.05 --> series is stationary, but seasonal MA from PACF
# diff by 12 lags to remove seasonality
ddlu = diff(dlu, lag=12)
invisible(acf2(ddlu, 130))
# ACF: cutoff after 1 seasonal lag --> seasonal MA, max order 1
# ACF: cutoff after 3 lags --> MA, max order 3
# PACF: cutoff after 3 seasonal lags --> seasonal
# test null hypothesis of non-stationarity (unit root)
adf.test(ddlu)
# p =0.01 < 0.05, series is stationary
# plot transformed data
plot.ts(cbind(unemp.ts,dlu,ddlu), main='Monthly unemployment rate, January 1948 - June 2017')
# start w/ d=1, D=1 from ADF tests for stationarity, max of 3 seasonal and 3 non-seasonal lags from ACF, PACF
### 1. EDA for automotive sales
auto = read.csv("TOTALNSA.csv")
cbind(head(auto), tail(auto))
auto.ts = ts(auto$TOTALNSA, start = c(1976, 1), frequency=12)
# plot raw data
plot.ts(auto.ts, main = 'Monthly automotive sales, January 1976 - June 2017', ylab = 'Automotive sales (units)')
invisible(acf2(auto.ts, 130))  # ACF: trend and seasonality
# test null hypothesis of non-stationarity (unit root)
adf.test(auto.ts)  # p = 0.04 < 0.05, series is stationary, no transform needed
## 2. transform data to stationary series
# diff by 1 lag to remove monthly trend
dla = diff(auto.ts)
invisible(acf2(dla, 130))
# test null hypothesis of non-stationarity (unit root)
adf.test(dla)  # p = 0.01 < 0.05, series is stationary
# diff by 12 lags to remove seasonal trend
ddla = diff(dla, lag=12)
invisible(acf2(ddla, 250))
# test null hypothesis of non-stationarity (unit root)
adf.test(ddla)  # p = 0.01 < 0.05, series is stationary
# plot transformed data
plot.ts(cbind(auto.ts,dla,ddla), main='Monthly automotive sales, January 1976 - June 2017')
### 3.a.fit seasonal models and select final candidate models with lowest AIC/BIC
# split data into training and test sets
unemp.train = window(unemp.ts, end=c(2015,12), frequency=12)
unemp.test = window(unemp.ts, start=c(2016,1), frequency=12)
# try models w/ max 3 seasonal AR, MA lags based on ACF, PACF
# start w/ D=1 from EDA and transforms
for (P in 0:3) {
for(Q in 0:3) {
fit = Arima(unemp.train, order=c(0,1,0),
seasonal=list(order=c(P,1,Q)), method='ML')
print(c(P,Q,fit$aic))
}
}
# candidate seasonal models (P,D,Q):
# (0,1,1) AIC = 47.1 --> low AIC, largest decrease in AIC, most parsimonious model
# (2,1,3) AIC = 42.6 --> lowest AIC, not much improvement in AIC, more complex model
### 3.b. fit non-seasonal models (given seasonal models) and select final candidate models with lowest AIC/BIC
# try models w/ max of 3 non-seasonal AR, MA lags based on ACF, PACF
for (p in 0:3) {
for(q in 0:3) {
try(fit <- Arima(unemp.train, order=c(p,1,q),
seasonal=list(order=c(0,1,1)), method='ML'))
print(c(p,q,fit$aic))
}
}
# candidate non-seasonal models (p,d,q):
# (0,1,0) AIC = 47.1 --> baseline model
# (0,1,2) AIC = 1.6 --> low AIC, largest decrease in AIC, most parsimonious model
# (1,1,1) AIC = -12.7  --> low AIC, large decrease in AIC
# (1,1,2) AIC = -18.0  --> 2nd lowest AIC, slight improvement
# (2,1,1) AIC = -18.3  --> lowest AIC, slight improvement
# (2,1,2) AIC = -16.4
### 4. check candidate models for residuals ~ white noise
sarima_diag = function(train, p,d,q, P,D,Q) {
m = Arima(train, order=c(p,d,q),
seasonal=list(order=c(P,D,Q)), method='ML')
# residual diagnostics
par(mfcol=c(2,2))
hist(m$residuals)
qqnorm(m$residuals)
acf(m$residuals, 130)
pacf(m$residuals, 130)
}
# diagnostics for candidate models
sarima_diag(unemp.train, 0,1,0, 0,1,1)  # residuals are serially correlated, normally distributed
sarima_diag(unemp.train, 0,1,2, 0,1,1)  # residuals are serially uncorrelated, normally distributed
sarima_diag(unemp.train, 2,1,1, 0,1,1)  # residuals are serially uncorrelated, normally distributed
sarima_diag(unemp.train, 1,1,1, 0,1,1)  # residuals are serially uncorrelated, normally distributed
sarima_diag(unemp.train, 1,1,2, 0,1,1)  # residuals are serially uncorrelated, normally distributed
sarima_diag(unemp.train, 2,1,2, 0,1,1)  # residuals are serially uncorrelated, normally distributed
### 5. check candidate models for out of sample accuracy
# candidate models
m0 = Arima(unemp.train, order=c(0,1,0), seasonal=list(order=c(0,1,1)), method='ML')
m1 = Arima(unemp.train, order=c(0,1,2), seasonal=list(order=c(0,1,1)), method='ML')
m2 = Arima(unemp.train, order=c(1,1,1), seasonal=list(order=c(0,1,1)), method='ML')
m3 = Arima(unemp.train, order=c(1,1,2), seasonal=list(order=c(0,1,1)), method='ML')
m4 = Arima(unemp.train, order=c(2,1,1), seasonal=list(order=c(0,1,1)), method='ML')
m5 = Arima(unemp.train, order=c(2,1,2), seasonal=list(order=c(0,1,1)), method='ML')
# forecasts for test period
for0 = forecast(m0, h=length(unemp.test))
for1 = forecast(m1, h=length(unemp.test))
for2 = forecast(m2, h=length(unemp.test))
for3 = forecast(m3, h=length(unemp.test))
for4 = forecast(m4, h=length(unemp.test))
for5 = forecast(m5, h=length(unemp.test))
# check accuracy against test set
accuracy(for0, unemp.test)  # RMSE = 0.152
accuracy(for1, unemp.test)  # RMSE = 0.153
accuracy(for2, unemp.test)  # RMSE = 0.155
accuracy(for3, unemp.test)  # RMSE = 0.151 --> best fit, use SARIMA(1,1,2)(0,1,1) for final forecast
accuracy(for4, unemp.test)  # RMSE = 0.152
accuracy(for5, unemp.test)  # RMSE = 0.151
### 6.a. forecast through June 2017
# final forecast with model 3: SARIMA(1,1,2) (0,1,1) and CIs
for2y = sarima.for(unemp.train, length(unemp.test), 1,1,2, 0,1,1, 12)
lines(unemp.test, col='black')
### 6.a. forecast through December 2020
# final forecast with model 3: SARIMA(1,1,2) (0,1,1) and CIs
for5y = sarima.for(unemp.train, 60, 1,1,2, 0,1,1, 12)
lines(unemp.test, col='black')
tail(for5y)
### fit linear model w/ shorter window for data set, account for stochastic trend due to event
unemp$DATE <- as.Date(unemp$DATE)
unemp$time <- rank(unemp$DATE)
unemp$month <- months(unemp$DATE)
unemp_test <- unemp[unemp$DATE>"2015-12-01",]
unemp_train <- unemp[unemp$DATE<"2016-01-01",]
mod1 <- lm(UNRATENSA ~ time + month, data = unemp_train)
summary(mod1)
# check residuals - normally distributed (plot, histogram, Q-Q), white noise (ACF, PACF)
par(mfrow = c(2,2))
plot(mod1$residuals, type = "l")
qqnorm(mod1$residuals)
acf(mod1$residuals, lag.max = 48)
pacf(mod1$residuals, lag.max = 48)
# plot fitted values on training set
par(mfrow = c(1,1))
plot(unemp_train$UNRATENSA~unemp_train$time, type = "l") + lines(predict(mod1,newdata = unemp_train), col="blue")
# forecast input data
test <- data.frame(time = 817:876, month = rep(c("January", "February", "March",
"April", "May", "June", "July",
"August", "September", "October",
"November", "December"),5))
##plot predictions
plot(predict(mod1, newdata = test[1:18,]), type="l", ylab="Unemployment Rate", ylim=c(4,8))
lines(unemp_test$UNRATENSA, col="blue")
### forecast through June 2017
accuracy(predict(mod1, newdata = test[1:18,]),unemp_test$UNRATENSA)
### forecast through December 2020
accuracy(predict(mod1, newdata = test[1:60,]),unemp_test$UNRATENSA)
mod2 <- lm(UNRATENSA ~ time + month, data = unemp_train[unemp$DATE>"2009-01-01",])
summary(mod2)
# check residuals - normally distributed (plot, histogram, Q-Q), white noise (ACF, PACF)
par(mfrow = c(2,2))
plot(mod2$residuals, type = "l")
qqnorm(mod2$residuals)
acf(mod2$residuals, lag.max = 48)
pacf(mod2$residuals, lag.max = 48)
# plot fitted values on training set
par(mfrow = c(1,1))
plot(unemp_train[unemp$DATE>"2009-01-01",]$UNRATENSA~unemp_train[unemp$DATE>"2009-01-01",]$time,
type = "l") + lines(predict(mod2,newdata = unemp_train), col="blue")
# forecast input data
test <- data.frame(time = 817:876, month = rep(c("January", "February", "March",
"April", "May", "June", "July",
"August", "September", "October",
"November", "December"),5))
##plot predictions
plot(predict(mod2, newdata = test[1:18,]), type="l", ylab="Unemployment Rate", ylim=c(4,8))
lines(unemp_test$UNRATENSA, col="blue")
### forecast through June 2017
accuracy(predict(mod2, newdata = test[1:18,]),unemp_test$UNRATENSA)
### forecast through December 2020
accuracy(predict(mod2, newdata = test[1:60,]),unemp_test$UNRATENSA)
### fit linear model w/ shorter window for data set, account for stochastic trend due to event
unemp$DATE <- as.Date(unemp$DATE)
unemp$time <- rank(unemp$DATE)
unemp$month <- months(unemp$DATE)
unemp_test <- unemp[unemp$DATE>"2015-12-01",]
unemp_train <- unemp[unemp$DATE<"2016-01-01",]
mod1 <- lm(UNRATENSA ~ time + month, data = unemp_train)
summary(mod1)
# check residuals - normally distributed (plot, histogram, Q-Q), white noise (ACF, PACF)
par(mfrow = c(2,2))
plot(mod1$residuals, type = "l")
qqnorm(mod1$residuals)
acf(mod1$residuals, lag.max = 48)
pacf(mod1$residuals, lag.max = 48)
# plot fitted values on training set
par(mfrow = c(1,1))
plot(unemp_train$UNRATENSA~unemp_train$time, type = "l") + lines(predict(mod1,newdata = unemp_train), col="blue", main='Unemployment rate actuals and fitted values for linear model')
# forecast input data
test <- data.frame(time = 817:876, month = rep(c("January", "February", "March",
"April", "May", "June", "July",
"August", "September", "October",
"November", "December"),5))
##plot predictions
plot(predict(mod1, newdata = test[1:18,]), type="l", ylab="Unemployment Rate", ylim=c(4,8), main='Unemployment rate forecast for linear model')
lines(unemp_test$UNRATENSA, col="blue")
### forecast through June 2017
accuracy(predict(mod1, newdata = test[1:18,]),unemp_test$UNRATENSA)
### forecast through December 2020
accuracy(predict(mod1, newdata = test[1:60,]),unemp_test$UNRATENSA)
mod2 <- lm(UNRATENSA ~ time + month, data = unemp_train[unemp$DATE>"2009-01-01",])
summary(mod2)
# check residuals - normally distributed (plot, histogram, Q-Q), white noise (ACF, PACF)
par(mfrow = c(2,2))
plot(mod2$residuals, type = "l")
qqnorm(mod2$residuals)
acf(mod2$residuals, lag.max = 48)
pacf(mod2$residuals, lag.max = 48)
# plot fitted values on training set
par(mfrow = c(1,1))
plot(unemp_train[unemp$DATE>"2009-01-01",]$UNRATENSA~unemp_train[unemp$DATE>"2009-01-01",]$time,
type = "l") + lines(predict(mod2,newdata = unemp_train), col="blue", main='Unemployment rate actuals and fitted values for linear model after 2009')
# forecast input data
test <- data.frame(time = 817:876, month = rep(c("January", "February", "March",
"April", "May", "June", "July",
"August", "September", "October",
"November", "December"),5))
## plot predictions
plot(predict(mod2, newdata = test[1:18,]), type="l", ylab="Unemployment Rate", ylim=c(4,8), main='Unemployment rate forecast for linear model after 2009')
lines(unemp_test$UNRATENSA, col="blue")
### forecast through June 2017
accuracy(predict(mod2, newdata = test[1:18,]),unemp_test$UNRATENSA)
### forecast through December 2020
accuracy(predict(mod2, newdata = test[1:60,]),unemp_test$UNRATENSA)
# split data into training and test sets
unemp.train2 = window(unemp.train, start=c(1976,1), end=c(2015,12), frequency=12)
unemp.test = window(unemp.ts, start=c(2016,1), frequency=12)
auto.train = window(auto.ts, end=c(2015,12), frequency=12)
auto.test = window(auto.ts, start=c(2016,1), frequency=12)
un_car = cbind(unemp.train2, auto.train)
plot(un_car)
# use ddlu and ddla to examine if the differenced data is stationary
ddlu.train = window(ddlu, start=c(1977,2), end=c(2015,12), frequency=12)
ddlu.test = window(ddlu, start=c(2016,1), frequency=12)
ddla.train = window(ddla, end=c(2015,12), frequency=12)
ddla.test = window(ddla, start=c(2016,1), frequency=12)
# differenced and seasonally differenced data is stationary
adf.test(ddlu.train)  # p = 0.01 < 0.05, data is stationary
pp.test(ddlu.train)  # p = 0.01 < 0.05, data is stationary
adf.test(ddla.train)  # p = 0.01 < 0.05, data is stationary
pp.test(ddla.train)  # p = 0.01 < 0.05, data is stationary
# ccf for original data and differenced data
ccf(unemp.train2, auto.train)
ccf(ddlu.train, ddla.train)
# examine optimal VAR order by AIC, p = 13 shows smallest AIC
VARselect(un_car, type='both', lag.max = 30, season=12)
# season is assigned. QUESTION: should we also assign const or trend?
var1 <- VAR(un_car, p = 1, type='both', season = 12)
var13 <- VAR(un_car, p = 13, type='both', season = 12)
invisible(acf2(residuals(var1)[,1], 200))
invisible(acf2(residuals(var1)[,2], 200))
ccf(residuals(var1)[,1], residuals(var1)[,2])
# var13, apparently, var13 beats var1
invisible(acf2(residuals(var13)[,1], 200))
invisible(acf2(residuals(var13)[,2], 200))
ccf(residuals(var13)[,1], residuals(var13)[,2])
### Forecasts
f1 <- predict(var1, n.ahead = length(unemp.test))
f13 <- predict(var13, n.ahead = length(auto.test))
# plot x.test vs forecasts, var13 shows best fitting, corresponding to residuals analysis
plot(as.vector(unemp.test), type = 'l')
lines(f1$fcst$unemp.train2[,1], col = 'red')
lines(f13$fcst$unemp.train2[,1], col = 'blue')
# forecast plots
plot(forecast(var13, 18), main='Unemployment rate and VAR13 forecast')
plot(forecast(var1, 18), main='Unemployment rate and VAR1 forecast')
# compare SARIMA and VAR
plot(as.vector(unemp.test), type = 'l')
lines(f13$fcst$unemp.train2[,1], col = 'blue')
lines(as.vector(for3$mean), col = 'red')
# check accuracy vs. SARIMA model
accuracy(f13$fcst$unemp.train2[,1], unemp.test)  # RMSE = 0.285
accuracy(for3, unemp.test)  # RMSE = 0.151
### Forecasts
f1 <- predict(var1, n.ahead = length(unemp.test))
f13 <- predict(var13, n.ahead = length(auto.test))
# plot x.test vs forecasts, var13 shows best fitting, corresponding to residuals analysis
plot(as.vector(unemp.test), type = 'l')
lines(f1$fcst$unemp.train2[,1], col = 'red')
lines(f13$fcst$unemp.train2[,1], col = 'blue')
# forecast plots
plot(forecast(var13, 18), main='Unemployment rate and VAR13 forecast')
plot(forecast(var1, 18), main='Unemployment rate and VAR1 forecast')
# compare SARIMA and VAR
plot(as.vector(unemp.test), type = 'l', main='Unemployment rate and SARIMA, VAR13 forecasts')
lines(f13$fcst$unemp.train2[,1], col = 'blue')
lines(as.vector(for3$mean), col = 'red')
# check accuracy vs. SARIMA model
accuracy(f13$fcst$unemp.train2[,1], unemp.test)  # RMSE = 0.285
accuracy(for3, unemp.test)  # RMSE = 0.151
